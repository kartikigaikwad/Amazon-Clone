{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObOwSoTR/A/oa//Bu/Skyg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kartikigaikwad/Amazon-Clone/blob/main/kartiki.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 1: Install & Setup ===\n",
        "!pip install -U openai==1.55.3 langchain langchain-openai langchain-core pinecone gradio arxiv tqdm\n",
        "\n",
        "import arxiv\n",
        "import openai\n",
        "import pinecone\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "from pinecone import Pinecone, ServerlessSpec, PineconeApiException\n",
        "import gradio as gr\n",
        "\n",
        "# Helper: Load secrets from Colab userdata (or manual env vars)\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = userdata.get(f\"{var}\")\n",
        "\n",
        "for var in [\"AZURE_OPENAI_API_KEY\", \"AZURE_OPENAI_ENDPOINT\", \"USER_AGENT\",\n",
        "            \"PINECONE_API_KEY\", \"PINECONE_INDEX\", \"OPENAI_API_VERSION\"]:\n",
        "    _set_env(var)\n",
        "\n",
        "openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
        "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        "openai.api_type = \"azure\"\n",
        "openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
        "\n",
        "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
        "pinecone_index_name = os.getenv(\"PINECONE_INDEX\")\n",
        "\n",
        "pinecone_client = Pinecone(api_key=pinecone_api_key)\n",
        "\n",
        "# Create index if not present\n",
        "try:\n",
        "    existing_indexes = [i['name'] for i in pinecone_client.list_indexes()]\n",
        "    if pinecone_index_name not in existing_indexes:\n",
        "        print(f\"Creating new Pinecone index: {pinecone_index_name}\")\n",
        "        pinecone_client.create_index(\n",
        "            name=pinecone_index_name,\n",
        "            dimension=1536,\n",
        "            metric=\"cosine\",\n",
        "            spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
        "        )\n",
        "    else:\n",
        "        print(f\"Pinecone index '{pinecone_index_name}' already exists.\")\n",
        "except PineconeApiException as e:\n",
        "    if e.status == 409 and \"ALREADY_EXISTS\" in e.body:\n",
        "        print(f\"Pinecone index '{pinecone_index_name}' already exists.\")\n",
        "    else:\n",
        "        raise\n",
        "\n",
        "index = pinecone_client.Index(pinecone_index_name)\n",
        "print(f\"‚úÖ Connected to Pinecone index: {pinecone_index_name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ehka4X135XB",
        "outputId": "af2ec56d-ab6f-42aa-d0b2-6919ebf2208c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==1.55.3 in /usr/local/lib/python3.12/dist-packages (1.55.3)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain\n",
            "  Using cached langchain-1.0.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (0.2.13)\n",
            "Collecting langchain-openai\n",
            "  Using cached langchain_openai-1.0.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (0.3.79)\n",
            "Collecting langchain-core\n",
            "  Using cached langchain_core-1.0.4-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: pinecone in /usr/local/lib/python3.12/dist-packages (7.3.0)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.49.1)\n",
            "Requirement already satisfied: arxiv in /usr/local/lib/python3.12/dist-packages (2.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.55.3) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.55.3) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.55.3) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.55.3) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.55.3) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai==1.55.3) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai==1.55.3) (4.15.0)\n",
            "Collecting langgraph<1.1.0,>=1.0.2 (from langchain)\n",
            "  Using cached langgraph-1.0.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "INFO: pip is looking at multiple versions of langchain-openai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-openai\n",
            "  Using cached langchain_openai-1.0.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Using cached langchain_openai-1.0.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Using cached langchain_openai-0.3.35-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Using cached langchain_openai-0.3.34-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Using cached langchain_openai-0.3.33-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Using cached langchain_openai-0.3.32-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Using cached langchain_openai-0.3.31-py3-none-any.whl.metadata (2.4 kB)\n",
            "INFO: pip is still looking at multiple versions of langchain-openai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached langchain_openai-0.3.30-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Using cached langchain_openai-0.3.29-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Using cached langchain_openai-0.3.28-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.27-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.26-py3-none-any.whl.metadata (2.3 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached langchain_openai-0.3.25-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.24-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.23-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.22-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.21-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.20-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.19-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.18-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.17-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.16-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.15-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.14-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.13-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.12-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.11-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.10-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.9-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.7-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.6-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.5-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.3-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Using cached langchain_openai-0.3.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Using cached langchain_openai-0.3.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Using cached langchain_openai-0.3.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Using cached langchain_openai-0.2.14-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Using cached langchain_openai-0.2.13-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Using cached langchain_openai-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Using cached langchain_openai-0.2.11-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Using cached langchain_openai-0.2.10-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.2.9-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.2.8-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.2.7-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.2.6-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.2.5-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.2.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.2.3-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.2.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.1.25-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.1.24-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.1.23-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.1.22-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.1.20-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.1.19-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.1.17-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.16-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.15-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.14-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.13-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.12-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.11-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.10-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.7-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.6-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.5-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.3-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.0.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.0.7-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.0.6-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.0.5-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.0.4-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.0.3-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.0.2.post1-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Using cached langchain_openai-0.0.2-py3-none-any.whl.metadata (570 bytes)\n",
            "Collecting langchain\n",
            "  Using cached langchain-1.0.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached langchain-1.0.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Using cached langchain-1.0.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Using cached langchain-1.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Using cached langchain-1.0.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.42)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.44)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (24.2)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2025.10.5)\n",
            "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from pinecone) (1.8.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from pinecone) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2.5.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.121.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.4)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.4)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.4)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.49.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
            "Requirement already satisfied: feedparser~=6.0.10 in /usr/local/lib/python3.12/dist-packages (from arxiv) (6.0.12)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai==1.55.3) (3.11)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.3)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.12/dist-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==1.55.3) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.55.3) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==1.55.3) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==1.55.3) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==1.55.3) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Pinecone index 'my-pro' already exists.\n",
            "‚úÖ Connected to Pinecone index: my-pro\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 2: Utility Functions ===\n",
        "import uuid\n",
        "import json\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "def search_arxiv(query: str, max_results: int = 5):\n",
        "    print(f\"\\nüîç Searching arXiv for '{query}' ...\")\n",
        "    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance)\n",
        "    results = []\n",
        "    for r in search.results():\n",
        "        results.append({\n",
        "            \"id\": r.get_short_id(),\n",
        "            \"title\": r.title,\n",
        "            \"authors\": [a.name for a in r.authors],\n",
        "            \"summary\": r.summary,\n",
        "            \"pdf_url\": r.pdf_url,\n",
        "            \"published\": r.published.isoformat() if r.published else None\n",
        "        })\n",
        "    print(f\"‚úÖ Found {len(results)} papers.\")\n",
        "    return results\n",
        "\n",
        "\n",
        "def chunk_texts(texts: List[str], chunk_size: int = 1000, overlap: int = 100):\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
        "    chunks = splitter.split_text(\"\\n\".join(texts))\n",
        "    print(f\"üß© Created {len(chunks)} text chunks.\")\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def get_embeddings_client(deployment=\"text-embedding-3-small\"):\n",
        "    return AzureOpenAIEmbeddings(deployment=deployment)\n",
        "\n",
        "\n",
        "def embed_texts(emb_client, texts: List[str], batch_size: int = 8):\n",
        "    print(\"‚öôÔ∏è Generating embeddings...\")\n",
        "    embeddings = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size)):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        batch_embs = emb_client.embed_documents(batch)\n",
        "        embeddings.extend(batch_embs)\n",
        "    print(f\"‚úÖ Created embeddings for {len(texts)} chunks.\")\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "def upsert_to_pinecone(index, embeddings, texts, namespace=\"default\"):\n",
        "    print(f\"üöÄ Uploading {len(embeddings)} vectors to Pinecone...\")\n",
        "    data = [(str(uuid.uuid4()), emb, {\"text\": texts[i]}) for i, emb in enumerate(embeddings)]\n",
        "    index.upsert(vectors=data, namespace=namespace)\n",
        "    print(\"‚úÖ Data successfully stored in Pinecone.\")\n"
      ],
      "metadata": {
        "id": "aB5Ga6dx4b-b"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 3: Full pipeline ===\n",
        "def index_research_topic(topic: str, max_papers: int = 5):\n",
        "    try:\n",
        "        logs = [f\"üöÄ Starting indexing for topic: {topic}\"]\n",
        "\n",
        "        # Step 1: Search arXiv\n",
        "        papers = search_arxiv(topic, max_results=max_papers)\n",
        "        if not papers:\n",
        "            return \"‚ùå No relevant papers found on arXiv.\"\n",
        "\n",
        "        logs.append(f\"‚úÖ Found {len(papers)} relevant papers.\")\n",
        "\n",
        "        # Step 2: Combine abstracts + titles\n",
        "        texts = [\n",
        "            f\"Title: {p['title']}\\nAuthors: {', '.join(p['authors'])}\\nAbstract: {p['summary']}\"\n",
        "            for p in papers\n",
        "        ]\n",
        "\n",
        "        # Step 3: Chunk text\n",
        "        chunks = chunk_texts(texts)\n",
        "        logs.append(f\"üß© Split into {len(chunks)} chunks.\")\n",
        "\n",
        "        # Step 4: Embedding generation\n",
        "        emb_client = get_embeddings_client()\n",
        "        embeddings = embed_texts(emb_client, chunks)\n",
        "        logs.append(\"‚úÖ Embeddings created successfully.\")\n",
        "\n",
        "        # Step 5: Store in Pinecone\n",
        "        upsert_to_pinecone(index, embeddings, chunks)\n",
        "        logs.append(\"üì¶ Data stored in Pinecone vector database.\")\n",
        "\n",
        "        logs.append(f\"üéØ Indexing complete for '{topic}'.\")\n",
        "        return \"\\n\".join(logs)\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error during indexing: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "s6C7axO64eCM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 4: Gradio App ===\n",
        "def gradio_index(topic, max_papers):\n",
        "    if not topic.strip():\n",
        "        return \"‚ö†Ô∏è Please enter a research topic.\"\n",
        "    return index_research_topic(topic, int(max_papers))\n",
        "\n",
        "app = gr.Interface(\n",
        "    fn=gradio_index,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Enter Research Topic\", placeholder=\"e.g. Transformer models in NLP\"),\n",
        "        gr.Slider(1, 10, step=1, value=3, label=\"Number of Papers to Retrieve\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"System Logs\", lines=15),\n",
        "    title=\"üìö Agentic Research Assistant ‚Äì Part 1\",\n",
        "    description=\"Enter a research topic to fetch papers from arXiv, create embeddings, and store them in Pinecone.\"\n",
        ")\n",
        "\n",
        "app.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "zHwAkRaa4jYk",
        "outputId": "e4a960b0-0adb-4acc-ab7f-e044c368402d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://4fa673d29c66775dd5.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4fa673d29c66775dd5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 5: Verify Pinecone Data (fixed for new SDK) ===\n",
        "def verify_pinecone(index):\n",
        "    print(\"\\nüîé Verifying Pinecone Index Contents...\")\n",
        "    stats = index.describe_index_stats()\n",
        "\n",
        "    # Convert to dict safely\n",
        "    if hasattr(stats, \"to_dict\"):\n",
        "        stats_dict = stats.to_dict()\n",
        "    elif isinstance(stats, dict):\n",
        "        stats_dict = stats\n",
        "    else:\n",
        "        # Fallback: convert via __dict__ (some versions return an object)\n",
        "        stats_dict = json.loads(json.dumps(stats, default=lambda o: o.__dict__))\n",
        "\n",
        "    # Pretty-print stats\n",
        "    print(json.dumps(stats_dict, indent=2))\n",
        "\n",
        "    # Extract total vector count (handling missing namespaces)\n",
        "    namespaces = stats_dict.get(\"namespaces\", {})\n",
        "    total = sum([ns.get(\"vector_count\", 0) for ns in namespaces.values()])\n",
        "    print(f\"\\n‚úÖ Total vectors stored in Pinecone: {total}\")\n",
        "\n",
        "# Call verification\n",
        "verify_pinecone(index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4APVrPg5MJV",
        "outputId": "e374f413-ea5c-43da-8b1c-ab984f7668e9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîé Verifying Pinecone Index Contents...\n",
            "{\n",
            "  \"namespaces\": {\n",
            "    \"default\": {\n",
            "      \"vector_count\": 39\n",
            "    }\n",
            "  },\n",
            "  \"index_fullness\": 0.0,\n",
            "  \"total_vector_count\": 39,\n",
            "  \"dimension\": 1536,\n",
            "  \"metric\": \"cosine\",\n",
            "  \"vector_type\": \"dense\"\n",
            "}\n",
            "\n",
            "‚úÖ Total vectors stored in Pinecone: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 6: Semantic Search Query ===\n",
        "def semantic_search(query: str, top_k: int = 3):\n",
        "    emb_client = get_embeddings_client()\n",
        "    q_emb = emb_client.embed_query(query)\n",
        "    results = index.query(vector=q_emb, top_k=top_k, include_metadata=True)\n",
        "    print(f\"\\nüîç Query: {query}\")\n",
        "    for match in results[\"matches\"]:\n",
        "        snippet = match[\"metadata\"][\"text\"][:200].replace(\"\\n\", \" \")\n",
        "        print(f\"\\nScore: {round(match['score'], 4)}\")\n",
        "        print(f\"Snippet: {snippet}...\")\n",
        "\n",
        "semantic_search(\"how CNN helps in detecting crop leaf diseases\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvfijIC25P3Z",
        "outputId": "438a50e8-6391-4cda-984d-b3fdb8ee5979"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Query: how CNN helps in detecting crop leaf diseases\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 6: Enhanced Topic-wise Namespace Version ===\n",
        "import re\n",
        "\n",
        "def clean_namespace(name: str) -> str:\n",
        "    \"\"\"Clean topic name so it can be used safely as Pinecone namespace.\"\"\"\n",
        "    return re.sub(r'[^a-zA-Z0-9_-]', '_', name.strip().lower())\n",
        "\n",
        "def index_research_topic_topicwise(topic: str, max_papers: int = 5):\n",
        "    \"\"\"Same as before, but stores data topic-wise and shows paper info.\"\"\"\n",
        "    try:\n",
        "        namespace = clean_namespace(topic)\n",
        "        logs = [f\"üöÄ Starting indexing for topic: '{topic}' (namespace: {namespace})\"]\n",
        "\n",
        "        # Step 1: Search arXiv\n",
        "        papers = search_arxiv(topic, max_results=max_papers)\n",
        "        if not papers:\n",
        "            return \"‚ùå No relevant papers found on arXiv.\"\n",
        "\n",
        "        logs.append(f\"‚úÖ Found {len(papers)} relevant papers.\\n\")\n",
        "\n",
        "        # Step 2: Display retrieved papers\n",
        "        for p in papers:\n",
        "            logs.append(f\"üìÑ **{p['title']}**\")\n",
        "            logs.append(f\"   üîó {p['pdf_url']}\")\n",
        "            logs.append(f\"   üßë‚Äçüî¨ {', '.join(p['authors'])}\")\n",
        "            logs.append(f\"   üìù {p['summary'][:250]}...\\n\")\n",
        "\n",
        "        # Step 3: Prepare text for embeddings\n",
        "        texts = [\n",
        "            f\"Title: {p['title']}\\nAuthors: {', '.join(p['authors'])}\\nAbstract: {p['summary']}\"\n",
        "            for p in papers\n",
        "        ]\n",
        "\n",
        "        # Step 4: Chunk text\n",
        "        chunks = chunk_texts(texts)\n",
        "        logs.append(f\"üß© Split into {len(chunks)} chunks.\")\n",
        "\n",
        "        # Step 5: Create embeddings\n",
        "        emb_client = get_embeddings_client()\n",
        "        embeddings = embed_texts(emb_client, chunks)\n",
        "        logs.append(\"‚úÖ Embeddings created successfully.\")\n",
        "\n",
        "        # Step 6: Store topic data in its own namespace\n",
        "        upsert_to_pinecone(index, embeddings, chunks, namespace=namespace)\n",
        "        logs.append(f\"üì¶ Data stored in Pinecone namespace '{namespace}'.\")\n",
        "\n",
        "        # Step 7: Verify & summarize\n",
        "        stats = index.describe_index_stats()\n",
        "        ns_count = stats['namespaces'].get(namespace, {}).get('vector_count', 0)\n",
        "        logs.append(f\"üîç Namespace '{namespace}' now contains {ns_count} vectors.\")\n",
        "\n",
        "        logs.append(f\"üéØ Indexing complete for topic '{topic}'.\")\n",
        "        return \"\\n\".join(logs)\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "tEUxAt9nEjWp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 7: Topic-wise Gradio Interface ===\n",
        "def gradio_index_topicwise(topic, max_papers):\n",
        "    if not topic.strip():\n",
        "        return \"‚ö†Ô∏è Please enter a research topic.\"\n",
        "    return index_research_topic_topicwise(topic, int(max_papers))\n",
        "\n",
        "app2 = gr.Interface(\n",
        "    fn=gradio_index_topicwise,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Enter Research Topic\", placeholder=\"e.g. Diffusion models in image generation\"),\n",
        "        gr.Slider(1, 10, step=1, value=3, label=\"Number of Papers to Retrieve\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"System Logs\", lines=20),\n",
        "    title=\"üìö Agentic Research Assistant ‚Äì Topic-wise Namespace Indexing\",\n",
        "    description=\"This version retrieves papers from arXiv, shows summaries, and stores each topic in its own Pinecone namespace.\"\n",
        ")\n",
        "\n",
        "app2.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "FLiGaJDKElA2",
        "outputId": "85ee3f87-a51d-4707-ae42-9368a7a302f8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b9c46417e0c5840375.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b9c46417e0c5840375.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 8: Verify Topic-wise Namespaces (Fixed for new Pinecone SDK) ===\n",
        "def show_all_namespaces(index):\n",
        "    # Convert to a plain Python dict first\n",
        "    stats = index.describe_index_stats().to_dict() if hasattr(index.describe_index_stats(), 'to_dict') else index.describe_index_stats()\n",
        "\n",
        "    # Print raw JSON\n",
        "    print(json.dumps(stats, indent=2))\n",
        "\n",
        "    # Safely access namespaces\n",
        "    namespaces = stats.get(\"namespaces\", {})\n",
        "    if not namespaces:\n",
        "        print(\"\\n‚ö†Ô∏è No namespaces found. Try indexing a topic first.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nüìä Namespace Summary:\")\n",
        "    for ns, data in namespaces.items():\n",
        "        count = data.get(\"vector_count\", 0)\n",
        "        print(f\"  üß≠ {ns}: {count} vectors\")\n",
        "\n",
        "# ‚úÖ Run the function\n",
        "show_all_namespaces(index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaYcQQNrE2Z5",
        "outputId": "1c918d8d-156c-4443-8ed7-03cb982eed44"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"namespaces\": {\n",
            "    \"human_life\": {\n",
            "      \"vector_count\": 18\n",
            "    },\n",
            "    \"default\": {\n",
            "      \"vector_count\": 57\n",
            "    },\n",
            "    \"generative_ai\": {\n",
            "      \"vector_count\": 13\n",
            "    }\n",
            "  },\n",
            "  \"index_fullness\": 0.0,\n",
            "  \"total_vector_count\": 88,\n",
            "  \"dimension\": 1536,\n",
            "  \"metric\": \"cosine\",\n",
            "  \"vector_type\": \"dense\"\n",
            "}\n",
            "\n",
            "üìä Namespace Summary:\n",
            "  üß≠ human_life: 18 vectors\n",
            "  üß≠ default: 57 vectors\n",
            "  üß≠ generative_ai: 13 vectors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dFR8zEKbFJ69"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}